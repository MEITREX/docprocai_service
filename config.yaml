# URL to the graphql endpoint of the media service. This is used to fetch information about media records the service
# needs to process
#media_service_url: "http://localhost:3500/v1.0/invoke/docprocai-service/method/graphql"

media_service_url: "http://app-media:3001/graphql"

# Settings pertaining to the database connection of the service. Note that the service expects a postgresql database
# with the pgvector extension installed
database:
  # Connection string to the database
  connection_string: "user=root password=root host=database port=5432 dbname=docprocai_service"

# Settings pertaining to the transcript generation for videos
transcript_generation:
  # Name or path to folder containing the whisper model to use.
  whisper_model: "base"
  # Which device to use for inference. Can be "cpu" or "cuda"
  device: "cpu"

# Settings pertaining to the video segmentation step during the content processing pipeline
video_segmentation:
  # The threshold of how similar the current frame of the video has to be compared to the last to be considered
  # the same. If actual similarity is lower, the video is split into two segments at this position
  # Default: 0.9
  # Range: [0, 1]
  segment_image_similarity_threshold: 0.9
  # The minimum length a segment has to be in seconds. If the video's screen changes before the minimum segment length
  # has been reached, the change is ignored
  minimum_segment_length: 15


text_embedding:
  protocol: "http"
  hostname: "129.69.217.248"
  port: 11435
  dimensionality: 1024
  model_path: "/"

# Settings pertaining to the content linking step during the content processing pipeline. Content linking is the process
# of analyzing all documents and videos of a content object and checking which parts of the documents should be linked,
# e.g. a timestamp in a lecture recording with the respective page in the lecture's PDF
content_linking:
  # This threshold is used for linking videos & documents by image similarity. It is the threshold of how similar the
  # current frame of the video/page of the document has to be compared to the other one for them to be considered the
  # same and be linked.
  # Default: 0.7
  # Range: [0, 1]
  linking_image_similarity_threshold: 0.7
  # The maximum number of comparison steps taken when comparing images for similarity. The higher the number, the more
  # accurate the comparison, but the more resources are used.
  # Default: 150
  linking_image_similarity_steps: 150
  # The scaling factor used for the resolution of the images when comparing them for similarity. Images are scaled by
  # this factor. The higher the factor, the longer the comparison takes (because there are more pixels in the image)
  # A higher scaling does not necessarily result in a more accurate comparison.
    # Default: 0.2
  linking_image_scaling_factor: 0.2
  # The maximum number of threads to use for the content linking processing step. This is the number of threads used
  # for the parallel processing of the content linking step. The higher the number, the more resources are used, but
  # the faster the processing is done. It makes sense to set this to a number slightly lower than the number of CPU
  # cores available on the machine, otherwise other processes might be slowed down.
  linking_processing_max_threads: 8

# Settings pertaining to the llm generation steps (title generation, summarization) during the content
# processing pipeline
lecture_llm_generator:
  # Details to connect to an OLLAMA instance providing LLMs via its REST interface.
  ollama:
    protocol: "http"
    hostname: "129.69.217.248"
    port: "11434"
  # If set to true, the models will be kept loaded in memory at all times. If false, models will be re-loaded each time
  # data is processed. Keeping models loaded at all times significantly increases RAM & VRAM usage, especially if
  # different models are used for title & summary generation, because this means memory of the machine needs to be large
  # enough to keep 2 models loaded simultaneously. If both the base model path and lora path of the two llm generators
  # are identical, the model is automatically only loaded once to save memory.
  keep_models_loaded: false

  # Settings pertaining to the generation of titles for video segments
  segment_title_generator:
    # If true, llm features are enabled in the processing pipeline. If false, these steps are skipped in the pipeline
    # and relevant data attributes replaced by placeholders or left unset
    # Default: true
    enabled: true
    # Hyperparameters passed to the model at inference-time
    hyperparameters:
      max_new_tokens: 2000
      temperature: 0.0
      repetition_penalty: 1.15
    # The prompt given to the model for this task. {json_input} is automatically replaced by a stringified JSON array
    # containing an entry for each segment extracted from the video. {json_schema} is automatically replaced by a
    # JSON schema definition of the expected output the LLM should give.
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|>

      You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

      I have an input JSON file I need to process. It contains an array, where each element is a snippet of a lecture video. Each element contains the keys "start_time", which denotes the start time of the snippet in seconds after video start, a "transcript" of the spoken text, and "screen_text", the text on screen as detected by OCR. The transcript and screen_text might contain inaccuracies due to the nature of STT and OCR. The video was split into snippets by detecting when the screen changes by a significant amount. Please create a JSON object which contains a property for each segment in the input, where the property key is the start_time of the segment, and the value is a string containing your suggested title for that segment. Choose high-quality and concise titles. If you want two back-to-back snippet to be considered as the same chapter, give them the same title in your JSON array. Remember to answer only with a JSON file. This is the input JSON:

      ```
      {json_input}
      ```<|eot_id|><|start_header_id|>assistant<|end_header_id|>

  # Settings pertaining to the generation of summaries for documents
  document_summary_generator:services:
  database:
    image: pgvector/pgvector:pg16
    command: -c 'max_connections=500'
    restart: unless-stopped
    expose:
      - 5432
    ports:
      - "5432:5432"
    volumes:
      - dbdata:/var/lib/postgresql/data
      - ./../docprocai_service/pg-init-scripts:/docker-entrypoint-initdb.d
    environment:
      - POSTGRES_USER=root
      - POSTGRES_PASSWORD=root
      - POSTGRES_CREATE_DB_DOCPROCAI_SERVICE=docprocai_service
  app-docprocai:
    build:
      context: ./../docprocai_service
      dockerfile: Dockerfile
    restart: unless-stopped
    container_name: docprocai_service
    volumes:
      - "./../docprocai_service/llm_data:/app/llm_data"
    ports:
      - "9900:9900"
      - "9901:9901"
    depends_on:
      - database
  dapr-docprocai:
    image: "daprio/daprd"
    command: [
      "./daprd",
      "--app-id", "docprocai_service",
      "--app-port", "9901",
      "--dapr-http-port", "9900",
      "--resources-path", "./components"
    ]
    volumes:
      - "./../docprocai_service/components/:/components" # Mount our components folder for the runtime to use. The mounted location must match the --resources-path argument.
    depends_on:
      - app-docprocai
      - redis
    network_mode: "service:app-docprocai"
  redis:
    image: "redis:alpine"
    expose:
      - "6379"

    # If true, llm features are enabled in the processing pipeline. If false, these steps are skipped in the pipeline
    # and relevant data attributes replaced by placeholders or left unset
    # Default: true
    enabled: true
    # path to the folder containing the llm base model, or the model's huggingface repository name which should be used
    # for inference
    base_model_path: "./llm_data/models/Meta-Llama-3.1-8B-Instruct"
    # path to the folder containing the llm lora adapter which should be used for inference
    lora_model_path: null
    # Hyperparameters passed to the model at inference-time
    hyperparameters:
      max_new_tokens: 300
      temperature: 0.0
      repetition_penalty: 1.1
      exponential_decay_length_penalty: !!python/tuple [300,2.5]
    # The prompt given to the model for this task. {text_input} is automatically replaced by the extracted text from
    # the document.
    # top_n = 1
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|>

      You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

      The input data:
      ```
      {text_input}
      ```

      I have some text extracted from the document of a lecture. Please create a very compact overview/table of contents about which topics are covered in this lecture. Your response must be under 800 characters in length.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

      Table of contents of the covered topics:
