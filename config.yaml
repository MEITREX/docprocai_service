# URL to the graphql endpoint of the media service. This is used to fetch information about media records the service
# needs to process
media_service_url: "http://app-media:3001/graphql"

# Settings pertaining to the database connection of the service. Note that the service expects a postgresql database
# with the pgvector extension installed
database:
  # Connection string to the database
  connection_string: "user=root password=root host=database port=5432 dbname=docprocai_service"

# Settings pertaining to the transcript generation for videos
transcript_generation:
  # Name or path to folder containing the whisper model to use.
  whisper_model: "base"
  # Which device to use for inference. Can be "cpu" or "cuda"
  device: "cpu"

# Settings pertaining to the video segmentation step during the content processing pipeline
video_segmentation:
  # The threshold of how similar the current frame of the video has to be compared to the last to be considered
  # the same. If actual similarity is lower, the video is split into two segments at this position
  # Default: 0.9
  # Range: [0, 1]
  segment_image_similarity_threshold: 0.9
  # The minimum length a segment has to be in seconds. If the video's screen changes before the minimum segment length
  # has been reached, the change is ignored
  minimum_segment_length: 15

# Settings pertaining to the text embedding generation step during the content processing pipeline
text_embedding:
  # Path (or huggingface repository name) of the sentence transformer model which should be used for generation of the
  # text embeddings of the ingested documents
  model_path: "./llm_data/models/gte-large-en-v1.5"
  # Dimensionality of the text embeddings generated by the model. This has to match the dimensionality of the model
  # specified in the model_path
  # If dimensionality is changed, the service will not be able to run unless a database purge is performed.
  dimensionality: 1024

# Settings pertaining to the content linking step during the content processing pipeline. Content linking is the process
# of analyzing all documents and videos of a content object and checking which parts of the documents should be linked,
# e.g. a timestamp in a lecture recording with the respective page in the lecture's PDF
content_linking:
  # This threshold is used for linking videos & documents by image similarity. It is the threshold of how similar the
  # current frame of the video/page of the document has to be compared to the other one for them to be considered the
  # same and be linked.
  # Default: 0.75
  # Range: [0, 1]
  linking_image_similarity_threshold: 0.75

# Settings pertaining to the llm generation steps (title generation, summarization) during the content
# processing pipeline
lecture_llm_generator:
  # If set to true, the models will be kept loaded in memory at all times. If false, models will be re-loaded each time
  # data is processed. Keeping models loaded at all times significantly increases RAM & VRAM usage, especially if
  # different models are used for title & summary generation, because this means memory of the machine needs to be large
  # enough to keep 2 models loaded simultaneously. If both the base model path and lora path of the two llm generators
  # are identical, the model is automatically only loaded once to save memory.
  keep_models_loaded: false
  # Settings pertaining to the generation of titles for video segments
  segment_title_generator:
    # If true, llm features are enabled in the processing pipeline. If false, these steps are skipped in the pipeline
    # and relevant data attributes replaced by placeholders or left unset
    # Default: true
    enabled: true
    # path to the folder containing the llm base model, or the model's huggingface repository name which should be used
    # for inference
    base_model_path: "./llm_data/models/Meta-Llama-3.1-8B-Instruct"
    # path to the folder containing the llm lora adapter which should be used for inference
    lora_model_path: "./llm_data/loras/llama-3-1-8B-instr-titles-full-sliding-20k"
    # Hyperparameters passed to the model at inference-time
    hyperparameters:
      max_new_tokens: 2000
      do_sample: false
      repetition_penalty: 1.15
    # The prompt given to the model for this task. {json_input} is automatically replaced by a stringified JSON array
    # containing an entry for each segment extracted from the video. {json_schema} is automatically replaced by a
    # JSON schema definition of the expected output the LLM should give.
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|>

      You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

      I have an input JSON file I need to process. It contains an array, where each element is a snippet of a lecture video. Each element contains the keys "start_time", which denotes the start time of the snippet in seconds after video start, a "transcript" of the spoken text, and "screen_text", the text on screen as detected by OCR. The transcript and screen_text might contain inaccuracies due to the nature of STT and OCR. The video was split into snippets by detecting when the screen changes by a significant amount. Please create a JSON object which contains a property for each segment in the input, where the property key is the start_time of the segment, and the value is a string containing your suggested title for that segment. Choose high-quality and concise titles. If you want two back-to-back snippet to be considered as the same chapter, give them the same title in your JSON array. Remember to answer only with a JSON file. This is the input JSON:

      ```
      {json_input}
      ```<|eot_id|><|start_header_id|>assistant<|end_header_id|>

  # Settings pertaining to the generation of summaries for documents
  document_summary_generator:
    # If true, llm features are enabled in the processing pipeline. If false, these steps are skipped in the pipeline
    # and relevant data attributes replaced by placeholders or left unset
    # Default: true
    enabled: true
    # path to the folder containing the llm base model, or the model's huggingface repository name which should be used
    # for inference
    base_model_path: "./llm_data/models/Meta-Llama-3.1-8B-Instruct"
    # path to the folder containing the llm lora adapter which should be used for inference
    lora_model_path: null
    # Hyperparameters passed to the model at inference-time
    hyperparameters:
      max_new_tokens: 300
      do_sample: false
      repetition_penalty: 1.1
      exponential_decay_length_penalty: !!python/tuple [300,2.5]
    # The prompt given to the model for this task. {text_input} is automatically replaced by the extracted text from
    # the document.
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|>

      You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

      The input data:
      ```
      {text_input}
      ```

      I have some text extracted from the document of a lecture. Please create a very compact overview/table of contents about which topics are covered in this lecture. Your response must be under 800 characters in length.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

      Table of contents of the covered topics: